{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25aa894",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51203cd8",
   "metadata": {},
   "source": [
    "## Problem （unicode1）: Understanding Unicode （1 point）\n",
    "\n",
    "```python\n",
    ">>> chr(0)\n",
    "'\\x00'\n",
    ">>> print(chr(0))\n",
    "\n",
    ">>> \"this is a test\" + chr(0) + \"string\"\n",
    "'this is a test\\x00string'\n",
    ">>> print(\"this is a test\" + chr(0) + \"string\")\n",
    "this is a teststring\n",
    ">>>\n",
    "```\n",
    "\n",
    "## Problem(unicode2): Unicode Encodings(3 points)\n",
    "\n",
    "### (a)\n",
    "\n",
    "- UTF-8 为变长编码，对 ASCII 字符（文本中常见）仅用 1 字节，存储和处理更高效；\n",
    "- UTF-8 兼容 ASCII，在多语言文本中适应性更强\n",
    "- 而 UTF-16/32 对 ASCII 文本冗余度高（UTF-16 至少 2 字节，UTF-32 固定 4 字节）。\n",
    "\n",
    "### (b)\n",
    "\n",
    "输入 日语或中文即可\n",
    "\n",
    "### (c)\n",
    "\n",
    "指的是不符合 UTF-8 规则的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c121b87",
   "metadata": {},
   "source": [
    "## 2.4 BPE Tokenizer Training\n",
    "\n",
    "\n",
    "\n",
    "- Vocabulary initialization\n",
    "- pre-tokenization\n",
    "- BPE merge\n",
    "- Special tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e756db5",
   "metadata": {},
   "source": [
    "### Problem: BPE Tokenizer Training (15 points)\n",
    "\n",
    "这里主要问题在于弄清楚类型都是什么，bytes如何操作\n",
    "\n",
    "- initial_bytes\n",
    "  - bytes([i])和 bytes(i)的差异！\n",
    "  - bytes(i)是i个\\0拼起来\n",
    "\n",
    "- _pre_tokenize\n",
    "  - 他的任务是，拆分出初步的pretokenize的结果\n",
    "  - 注意：按照special tokens 拆分开，不能跨越special token进行merge\n",
    "  - 返回pre_tokens，内容为{tuple(byte1, byte2, ... , bytek) : count}\n",
    "- train\n",
    "  - 重点是merge部分的算法，这里是主要时耗\n",
    "- 为什么采取分离的方式构建vocab?\n",
    "  - 这样只需要维护`merged`就可以了\n",
    "\n",
    "\n",
    "- 2025-10-20最新战报：最大堆修正失败，寄\n",
    "- 2025-10-28 重整旗鼓 over\n",
    "- 2025-10-29 64G机器还是OOM ToT\n",
    "\n",
    "- 2025-10-30 打算今天根据一个blog 抄他更改的思路自己实现最后的工业级别的结果\n",
    "\n",
    "- TODO: `Optionally (this could be a large time-investment),you can implement the key parts of your training method using some systems language, for instanceC++ (considercppyyfor this) or Rust (using PyO3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4a679",
   "metadata": {},
   "source": [
    "这里是我加速bpe_training的流程思路之类的\n",
    "> 算法的优化，数据结构的优化，并行(openmp)优化，cython优化，用c++实现关键代码和c++库的cython集成\n",
    "\n",
    "- 用yield流式读取chunk，一点一点读取\n",
    "- 为了利用多个CPU进行并行计算，最常见的就是使用多线程。但是由于python的GIL，python的多线程(threading)只能实现并发(concurrency)而不是并行(parallel)，python的多线程只适合I/O密集型任务，而不适合CPU密集型任务。对于CPU密集型任务，比较合适的是使用多进程(multiprocessing)。\n",
    "- 另外，我们的计算需要分成很多步骤，因此使用队列和生产者/消费者模式来实现是比较清晰的。具体来说我们的流程如下：\n",
    "  - 文档切割 利用文档分隔特殊字符串<|endoftext|>把原始文本切分成多个chunk，每个chunk包含一个或多个文档。结果放到chunk_queue。\n",
    "  - 正则匹配和词频统计 把chunk用<|endoftext|>切分成文档，再把文档切分成词，然后统计词频。处理的结果放到counter_queue。\n",
    "  - 词频合并 从counter_queue取出chunk的词频统计结果，然后进行合并，结果放到merged_queue。\n",
    "  - 最终合并 主进程把merged_queue里的统计结果进行合并。\n",
    "- 遇到类似的问题，采用 8-2配置用时40s，采用16-2配置用时30s，采用16-4配置用时 30s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5e8ec",
   "metadata": {},
   "source": [
    "### Problem （train_bpe_tinystories）: BPE Training on TinyStories （2 points）\n",
    "\n",
    "- 2025-10-29 改了下 pre-tokenize, 防止一下全放到内存里给干炸了\n",
    "- 跑完了，爽\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f501bff",
   "metadata": {},
   "source": [
    "### Problem （train_bpe_expts_owt）: BPE Training on OpenWeb Text （2 points）\n",
    "```\n",
    " uv run cs336_basics/train_bpe.py\n",
    "chunk_queu: 0, counter_queue: 0, merged_queue: 0\n",
    "chunk_queu: 36559, counter_queue: 3, merged_queue: 0\n",
    "chunk_queu: 67665, counter_queue: 1, merged_queue: 0\n",
    "chunk_queu: 97751, counter_queue: 1, merged_queue: 0\n",
    "chunk_queu: 126147, counter_queue: 2, merged_queue: 0\n",
    "chunk_queu: 153798, counter_queue: 3, merged_queue: 0\n",
    "chunk_queu: 145248, counter_queue: 0, merged_queue: 0\n",
    "chunk_queu: 132496, counter_queue: 1, merged_queue: 0\n",
    "chunk_queu: 119565, counter_queue: 4, merged_queue: 0\n",
    "chunk_queu: 106843, counter_queue: 4, merged_queue: 0\n",
    "chunk_queu: 95257, counter_queue: 0, merged_queue: 0\n",
    "chunk_queu: 82290, counter_queue: 2, merged_queue: 0\n",
    "chunk_queu: 69311, counter_queue: 3, merged_queue: 0\n",
    "chunk_queu: 56490, counter_queue: 2, merged_queue: 0\n",
    "chunk_queu: 43478, counter_queue: 0, merged_queue: 0\n",
    "chunk_queu: 30519, counter_queue: 0, merged_queue: 0\n",
    "chunk_queu: 17555, counter_queue: 1, merged_queue: 0\n",
    "chunk_queu: 4603, counter_queue: 3, merged_queue: 0\n",
    "chunk_queu: 0, counter_queue: 0, merged_queue: 0\n",
    "_pretokenize_and_count takes: 190.0569782579987\n",
    "merge 1000: 231.10991237199778\n",
    "merge 2000: 371.4132240429972\n",
    "merge 3000: 575.2559400539976\n",
    "merge 4000: 836.2143881919983\n",
    "merge 5000: 1154.4111030869972\n",
    "merge 6000: 1515.6526871149981\n",
    "merge 7000: 1920.8361723350026\n",
    "merge 8000: 2374.731511883001\n",
    "merge 9000: 2907.8827909429965\n",
    "merge time: 3333.8703679980026\n",
    "Training completed in 3552.53 seconds.\n",
    "Vocab size: 10000\n",
    "Longest token: b'----------------' (length=16)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed55a6",
   "metadata": {},
   "source": [
    "## 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953be9f1",
   "metadata": {},
   "source": [
    "### Problem （tokenizer）:Implementing the tokenizer （15 points）\n",
    "\n",
    "- 2025-10-30 差一个 Memory 限制没有过"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9d3f8",
   "metadata": {},
   "source": [
    "### Problem （tokenizer_experiments）:Experiments with tokenizers （4 points）\n",
    "\n",
    "TinyStories 的 tokenizer 在压缩效率（更低的 bytes/token）和处理速度（更高的 MB/s）上均显著优于 OpenWebText 的 tokenizer，且跨数据集时性能依然稳定。这可能与两个数据集的特性有关：TinyStories 多为短文本、简单词汇，其 tokenizer 可能更侧重 “紧凑编码”；而 OpenWebText 文本更复杂（长文本、多样词汇），tokenizer 可能为了覆盖更多复杂模式而牺牲了部分效率。\n",
    "如果你的需求是高效处理大规模文本（尤其是追求速度和压缩率），TinyStories 的 tokenizer 会是更优选择。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da11cbd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12706a2",
   "metadata": {},
   "source": [
    "## 3.3\n",
    "\n",
    "这里在教学 `einops`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0781f19",
   "metadata": {},
   "source": [
    "## 3.4\n",
    "\n",
    "本次作业在数学表示上要求使用列向量 ($y = Wx$)，但在实际代码实现中，您应该回归 PyTorch/NumPy 的行向量惯例 ($y = xW^T$) 来利用其内存优化。\n",
    "\n",
    "- 实现了 linear 和 embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118cb49b",
   "metadata": {},
   "source": [
    "## 3.5\n",
    "> 2025-10-31 有意义的一天\n",
    "> 可能重要原因之一是在GenAI实现过 MHA和RoPE和GQA\n",
    "\n",
    "- Root Mean Square Layer Normalization([ ] 为什么work，当时课上也没有说？还是我忘记了)\n",
    "- SiLU\n",
    "- SwiGLU\n",
    "- RoPE\n",
    "- attention\n",
    "- MHA\n",
    "\n",
    "- TransformerBlock\n",
    "- TransformerLM"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
