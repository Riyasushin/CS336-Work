{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25aa894",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51203cd8",
   "metadata": {},
   "source": [
    "## Problem （unicode1）: Understanding Unicode （1 point）\n",
    "\n",
    "```python\n",
    ">>> chr(0)\n",
    "'\\x00'\n",
    ">>> print(chr(0))\n",
    "\n",
    ">>> \"this is a test\" + chr(0) + \"string\"\n",
    "'this is a test\\x00string'\n",
    ">>> print(\"this is a test\" + chr(0) + \"string\")\n",
    "this is a teststring\n",
    ">>>\n",
    "```\n",
    "\n",
    "## Problem(unicode2): Unicode Encodings(3 points)\n",
    "\n",
    "### (a)\n",
    "\n",
    "- UTF-8 为变长编码，对 ASCII 字符（文本中常见）仅用 1 字节，存储和处理更高效；\n",
    "- UTF-8 兼容 ASCII，在多语言文本中适应性更强\n",
    "- 而 UTF-16/32 对 ASCII 文本冗余度高（UTF-16 至少 2 字节，UTF-32 固定 4 字节）。\n",
    "\n",
    "### (b)\n",
    "\n",
    "输入 日语或中文即可\n",
    "\n",
    "### (c)\n",
    "\n",
    "指的是不符合 UTF-8 规则的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c121b87",
   "metadata": {},
   "source": [
    "## 2.4 BPE Tokenizer Training\n",
    "\n",
    "\n",
    "\n",
    "- Vocabulary initialization\n",
    "- pre-tokenization\n",
    "- BPE merge\n",
    "- Special tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e756db5",
   "metadata": {},
   "source": [
    "### Problem: BPE Tokenizer Training (15 points)\n",
    "\n",
    "这里主要问题在于弄清楚类型都是什么，bytes如何操作\n",
    "\n",
    "- initial_bytes\n",
    "  - bytes([i])和 bytes(i)的差异！\n",
    "  - bytes(i)是i个\\0拼起来\n",
    "\n",
    "- _pre_tokenize\n",
    "  - 他的任务是，拆分出初步的pretokenize的结果\n",
    "  - 注意：按照special tokens 拆分开，不能跨越special token进行merge\n",
    "  - 返回pre_tokens，内容为{tuple(byte1, byte2, ... , bytek) : count}\n",
    "- train\n",
    "  - 重点是merge部分的算法，这里是主要时耗\n",
    "- 为什么采取分离的方式构建vocab?\n",
    "  - 这样只需要维护`merged`就可以了\n",
    "\n",
    "\n",
    "- 2025-10-20最新战报：最大堆修正失败，寄\n",
    "- 2025-10-28 重整旗鼓 over\n",
    "- 2025-10-29 64G机器还是OOM ToT\n",
    "\n",
    "- 2025-10-30 打算今天根据一个blog 抄他更改的思路自己实现最后的工业级别的结果\n",
    "\n",
    "- TODO: `Optionally (this could be a large time-investment),you can implement the key parts of your training method using some systems language, for instanceC++ (considercppyyfor this) or Rust (using PyO3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4a679",
   "metadata": {},
   "source": [
    "这里是我加速bpe_training的流程思路之类的\n",
    "> 算法的优化，数据结构的优化，并行(openmp)优化，cython优化，用c++实现关键代码和c++库的cython集成\n",
    "\n",
    "- 用yield流式读取chunk，一点一点读取\n",
    "- 为了利用多个CPU进行并行计算，最常见的就是使用多线程。但是由于python的GIL，python的多线程(threading)只能实现并发(concurrency)而不是并行(parallel)，python的多线程只适合I/O密集型任务，而不适合CPU密集型任务。对于CPU密集型任务，比较合适的是使用多进程(multiprocessing)。\n",
    "- 另外，我们的计算需要分成很多步骤，因此使用队列和生产者/消费者模式来实现是比较清晰的。具体来说我们的流程如下：\n",
    "  - 文档切割 利用文档分隔特殊字符串<|endoftext|>把原始文本切分成多个chunk，每个chunk包含一个或多个文档。结果放到chunk_queue。\n",
    "  - 正则匹配和词频统计 把chunk用<|endoftext|>切分成文档，再把文档切分成词，然后统计词频。处理的结果放到counter_queue。\n",
    "  - 词频合并 从counter_queue取出chunk的词频统计结果，然后进行合并，结果放到merged_queue。\n",
    "  - 最终合并 主进程把merged_queue里的统计结果进行合并。\n",
    "- 遇到类似的问题，采用 8-2配置用时40s，采用16-2配置用时30s，采用16-4配置用时 30s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5e8ec",
   "metadata": {},
   "source": [
    "### Problem （train_bpe_tinystories）: BPE Training on TinyStories （2 points）\n",
    "\n",
    "- 2025-10-29 改了下 pre-tokenize, 防止一下全放到内存里给干炸了\n",
    "- 跑完了，爽\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f501bff",
   "metadata": {},
   "source": [
    "### Problem （train_bpe_expts_owt）: BPE Training on OpenWeb Text （2 points）\n",
    "```\n",
    " uv run cs336_basics/train_bpe.py\n",
    "chunk_queu: 0, counter_queue: 0, merged_queue: 0\n",
    "chunk_queu: 36559, counter_queue: 3, merged_queue: 0\n",
    "chunk_queu: 67665, counter_queue: 1, merged_queue: 0\n",
    "chunk_queu: 97751, counter_queue: 1, merged_queue: 0\n",
    "chunk_queu: 126147, counter_queue: 2, merged_queue: 0\n",
    "chunk_queu: 153798, counter_queue: 3, merged_queue: 0\n",
    "chunk_queu: 145248, counter_queue: 0, merged_queue: 0\n",
    "chunk_queu: 132496, counter_queue: 1, merged_queue: 0\n",
    "chunk_queu: 119565, counter_queue: 4, merged_queue: 0\n",
    "chunk_queu: 106843, counter_queue: 4, merged_queue: 0\n",
    "chunk_queu: 95257, counter_queue: 0, merged_queue: 0\n",
    "chunk_queu: 82290, counter_queue: 2, merged_queue: 0\n",
    "chunk_queu: 69311, counter_queue: 3, merged_queue: 0\n",
    "chunk_queu: 56490, counter_queue: 2, merged_queue: 0\n",
    "chunk_queu: 43478, counter_queue: 0, merged_queue: 0\n",
    "chunk_queu: 30519, counter_queue: 0, merged_queue: 0\n",
    "chunk_queu: 17555, counter_queue: 1, merged_queue: 0\n",
    "chunk_queu: 4603, counter_queue: 3, merged_queue: 0\n",
    "chunk_queu: 0, counter_queue: 0, merged_queue: 0\n",
    "_pretokenize_and_count takes: 190.0569782579987\n",
    "merge 1000: 231.10991237199778\n",
    "merge 2000: 371.4132240429972\n",
    "merge 3000: 575.2559400539976\n",
    "merge 4000: 836.2143881919983\n",
    "merge 5000: 1154.4111030869972\n",
    "merge 6000: 1515.6526871149981\n",
    "merge 7000: 1920.8361723350026\n",
    "merge 8000: 2374.731511883001\n",
    "merge 9000: 2907.8827909429965\n",
    "merge time: 3333.8703679980026\n",
    "Training completed in 3552.53 seconds.\n",
    "Vocab size: 10000\n",
    "Longest token: b'----------------' (length=16)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed55a6",
   "metadata": {},
   "source": [
    "## 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953be9f1",
   "metadata": {},
   "source": [
    "### Problem （tokenizer）:Implementing the tokenizer （15 points）\n",
    "\n",
    "- 2025-10-30 差一个 Memory 限制没有过"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9d3f8",
   "metadata": {},
   "source": [
    "### Problem （tokenizer_experiments）:Experiments with tokenizers （4 points）\n",
    "\n",
    "TinyStories 的 tokenizer 在压缩效率（更低的 bytes/token）和处理速度（更高的 MB/s）上均显著优于 OpenWebText 的 tokenizer，且跨数据集时性能依然稳定。这可能与两个数据集的特性有关：TinyStories 多为短文本、简单词汇，其 tokenizer 可能更侧重 “紧凑编码”；而 OpenWebText 文本更复杂（长文本、多样词汇），tokenizer 可能为了覆盖更多复杂模式而牺牲了部分效率。\n",
    "如果你的需求是高效处理大规模文本（尤其是追求速度和压缩率），TinyStories 的 tokenizer 会是更优选择。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da11cbd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12706a2",
   "metadata": {},
   "source": [
    "## 3.3\n",
    "\n",
    "这里在教学 `einops`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0781f19",
   "metadata": {},
   "source": [
    "## 3.4\n",
    "\n",
    "本次作业在数学表示上要求使用列向量 ($y = Wx$)，但在实际代码实现中，您应该回归 PyTorch/NumPy 的行向量惯例 ($y = xW^T$) 来利用其内存优化。\n",
    "\n",
    "- 实现了 linear 和 embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118cb49b",
   "metadata": {},
   "source": [
    "## 3.5\n",
    "> 2025-10-31 有意义的一天\n",
    "> 可能重要原因之一是在GenAI实现过 MHA和RoPE和GQA\n",
    "\n",
    "- Root Mean Square Layer Normalization([ ] 为什么work，当时课上也没有说？还是我忘记了)\n",
    "- SiLU\n",
    "- SwiGLU\n",
    "- RoPE\n",
    "- attention\n",
    "- MHA\n",
    "\n",
    "- TransformerBlock\n",
    "- TransformerLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72702a",
   "metadata": {},
   "source": [
    "## 3.5 transformer resource accounting\n",
    "\n",
    "\n",
    "### (A)\n",
    "2127057600 个 fp16 可训练\n",
    "\n",
    "8G左右\n",
    "\n",
    "### (B)\n",
    "\n",
    "RJTransformerLM模型FLOPs分析：\n",
    "  单块RMSNorm（注意力前）：6.55 MFLOPs\n",
    "  单块注意力总：27.69 GFLOPs\n",
    "  单块RMSNorm（FFN前）：6.55 MFLOPs\n",
    "  单块FFN总：62.93 GFLOPs\n",
    "  单块总FLOPs：90.63 GFLOPs\n",
    "  所有层总FLOPs：4.35 TFLOPs\n",
    "  输出层总FLOPs：164.69 GFLOPs\n",
    "  模型总FLOPs：4.52 TFLOPs\n",
    "\n",
    "### (C)\n",
    "单块FFN总：62.93 GFLOPs\n",
    "单块注意力总：27.69 GFLOPs\n",
    "\n",
    "\n",
    "### (D)\n",
    "\n",
    "---\n",
    "Small\n",
    "282472704\n",
    "模型FLOPs分析：\n",
    "  单块RMSNorm（注意力前）：3.15 MFLOPs\n",
    "  单块注意力总：8.06 GFLOPs\n",
    "  单块RMSNorm（FFN前）：3.15 MFLOPs\n",
    "  单块FFN总：30.21 GFLOPs\n",
    "  单块总FLOPs：38.28 GFLOPs\n",
    "  所有层总FLOPs：459.31 GFLOPs\n",
    "  输出层总FLOPs：79.05 GFLOPs\n",
    "  模型总FLOPs：538.36 GFLOPs\n",
    "\n",
    "---\n",
    "Medium\n",
    "675499008\n",
    "模型FLOPs分析：\n",
    "  单块RMSNorm（注意力前）：4.19 MFLOPs\n",
    "  单块注意力总：12.89 GFLOPs\n",
    "  单块RMSNorm（FFN前）：4.19 MFLOPs\n",
    "  单块FFN总：40.28 GFLOPs\n",
    "  单块总FLOPs：53.18 GFLOPs\n",
    "  所有层总FLOPs：1.28 TFLOPs\n",
    "  输出层总FLOPs：105.40 GFLOPs\n",
    "  模型总FLOPs：1.38 TFLOPs\n",
    "\n",
    "---\n",
    "Large\n",
    "1249416960\n",
    "模型FLOPs分析：\n",
    "  单块RMSNorm（注意力前）：5.24 MFLOPs\n",
    "  单块注意力总：18.80 GFLOPs\n",
    "  单块RMSNorm（FFN前）：5.24 MFLOPs\n",
    "  单块FFN总：50.34 GFLOPs\n",
    "  单块总FLOPs：69.15 GFLOPs\n",
    "  所有层总FLOPs：2.49 TFLOPs\n",
    "  输出层总FLOPs：131.75 GFLOPs\n",
    "  模型总FLOPs：2.62 TFLOPs\n",
    "\n",
    "### (E)\n",
    "\n",
    "\n",
    "2127057600\n",
    "模型FLOPs分析：\n",
    "  单块RMSNorm（注意力前）：104.86 MFLOPs\n",
    "  单块注意力总：2.05 TFLOPs\n",
    "  单块RMSNorm（FFN前）：104.86 MFLOPs\n",
    "  单块FFN总：1.01 TFLOPs\n",
    "  单块总FLOPs：3.06 TFLOPs\n",
    "  所有层总FLOPs：146.92 TFLOPs\n",
    "  输出层总FLOPs：2.64 TFLOPs\n",
    "  模型总FLOPs：149.55 TFLOPs\n",
    "\n",
    "我勒个平方级\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6cac533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2127057600\n",
      "模型FLOPs分析：\n",
      "  单块RMSNorm（注意力前）：104.86 MFLOPs\n",
      "  单块注意力总：2.05 TFLOPs\n",
      "  单块RMSNorm（FFN前）：104.86 MFLOPs\n",
      "  单块FFN总：1.01 TFLOPs\n",
      "  单块总FLOPs：3.06 TFLOPs\n",
      "  所有层总FLOPs：146.92 TFLOPs\n",
      "  输出层总FLOPs：2.64 TFLOPs\n",
      "  模型总FLOPs：149.55 TFLOPs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from cs336_basics.my_module import *\n",
    "\n",
    "# 格式化输出（转换为易读单位）\n",
    "def format_flops(value):\n",
    "    for unit in ['FLOPs', 'kFLOPs', 'MFLOPs', 'GFLOPs', 'TFLOPs', 'PFLOPs']:\n",
    "        if value < 1000:\n",
    "            return f\"{value:.2f} {unit}\"\n",
    "        value /= 1000\n",
    "    return f\"{value:.2f} PFLOPs\"\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    \"\"\"\n",
    "    统计模型中可训练参数的总数量\n",
    "    \n",
    "    参数:\n",
    "        model: PyTorch模型实例\n",
    "        \n",
    "    返回:\n",
    "        int: 可训练参数的总数量\n",
    "    \"\"\"\n",
    "    # 遍历模型所有参数，筛选出requires_grad=True的参数，累加每个参数的元素数量\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def calcu(\n",
    "    vocab_size: int,\n",
    "    context_length: int,\n",
    "    num_layers: int,\n",
    "    d_model: int,\n",
    "    num_heads: int,\n",
    "    d_ff: int\n",
    "):\n",
    "    \n",
    "    model = RJTransformerLM(\n",
    "        vocab_size=vocab_size,\n",
    "        context_length=context_length,\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        rope_theta=10000\n",
    "    )\n",
    "    print(count_trainable_params(model))\n",
    "    \n",
    "    #############################################################\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    L = context_length  # 序列长度\n",
    "    D = d_model         # 隐状态维度\n",
    "    H = num_heads       # 注意力头数\n",
    "    F = d_ff            # FFN中间维度\n",
    "    V = vocab_size      # 词汇表大小\n",
    "    d_k = D // H        # 每个注意力头的维度（D必须被H整除）\n",
    "\n",
    "    # --------------------------\n",
    "    # 1. 单个Transformer块的FLOPs\n",
    "    # --------------------------\n",
    "    per_block = {}\n",
    "\n",
    "    # 1.1 注意力层的RMSNorm\n",
    "    # 计算：元素平方（D*L） + 均值（D*L加法） + 缩放（D*L乘法+除法）\n",
    "    rms_attn = 4 * D * L  # 简化近似：4次操作/元素（平方、加法、除法、乘法）\n",
    "\n",
    "    # 1.2 多头自注意力\n",
    "    # Q/K/V投影（3个线性层）：每个FLOPs=2*D*D*L（矩阵乘法）\n",
    "    attn_qkv = 3 * 2 * D * D * L\n",
    "    # RoPE旋转（Q和K各一次）：6*L*D（元素级乘加，之前推导）\n",
    "    attn_rope = 6 * L * D\n",
    "    # Q×K^T：2*H*L^2*d_k = 2*L^2*D（因H*d_k=D）\n",
    "    attn_qk = 2 * (L **2) * D\n",
    "    # 注意力权重×V：2*L^2*D（同Q×K^T）\n",
    "    attn_v = 2 * (L** 2) * D\n",
    "    # 输出投影（线性层）：2*D*D*L\n",
    "    attn_out = 2 * D * D * L\n",
    "    # 注意力总FLOPs\n",
    "    attn_total = attn_qkv + attn_rope + attn_qk + attn_v + attn_out\n",
    "\n",
    "    # 1.3 FFN层的RMSNorm（同注意力层的RMSNorm）\n",
    "    rms_ffn = 4 * D * L\n",
    "\n",
    "    # 1.4 SwiGLU前馈网络\n",
    "    # W1和W3投影：2*D*F*L  each → 共4*D*F*L\n",
    "    ffn_w1w3 = 4 * D * F * L\n",
    "    # SiLU激活 + 逐元素相乘：2*F*L（每个元素2次操作）\n",
    "    ffn_silu_mul = 2 * F * L\n",
    "    # W2投影：2*F*D*L\n",
    "    ffn_w2 = 2 * F * D * L\n",
    "    # FFN总FLOPs\n",
    "    ffn_total = ffn_w1w3 + ffn_silu_mul + ffn_w2\n",
    "\n",
    "    # 单个块的总FLOPs\n",
    "    per_block_total = rms_attn + attn_total + rms_ffn + ffn_total\n",
    "\n",
    "    # --------------------------\n",
    "    # 2. 所有层的总FLOPs\n",
    "    # --------------------------\n",
    "    total_layers = num_layers * per_block_total\n",
    "\n",
    "    # --------------------------\n",
    "    # 3. 输出层FLOPs\n",
    "    # --------------------------\n",
    "    # 最终RMSNorm\n",
    "    output_rms = 4 * D * L\n",
    "    # 输出线性投影（D→V）\n",
    "    output_proj = 2 * D * V * L\n",
    "    output_total = output_rms + output_proj\n",
    "\n",
    "    # --------------------------\n",
    "    # 总FLOPs\n",
    "    # --------------------------\n",
    "    total_flops = total_layers + output_total\n",
    "    \n",
    "    print(\"模型FLOPs分析：\")\n",
    "    print(f\"  单块RMSNorm（注意力前）：{format_flops(rms_attn)}\")\n",
    "    print(f\"  单块注意力总：{format_flops(attn_total)}\")\n",
    "    print(f\"  单块RMSNorm（FFN前）：{format_flops(rms_ffn)}\")\n",
    "    print(f\"  单块FFN总：{format_flops(ffn_total)}\")\n",
    "    print(f\"  单块总FLOPs：{format_flops(per_block_total)}\")\n",
    "    print(f\"  所有层总FLOPs：{format_flops(total_layers)}\")\n",
    "    print(f\"  输出层总FLOPs：{format_flops(output_total)}\")\n",
    "    print(f\"  模型总FLOPs：{format_flops(total_flops)}\")\n",
    "\n",
    "# 模型参数（用户提供）\n",
    "params = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 16384,\n",
    "    \"num_layers\": 48,\n",
    "    \"d_model\": 1600,\n",
    "    \"num_heads\": 25,\n",
    "    \"d_ff\": 6400\n",
    "}\n",
    "\n",
    "# 计算FLOPs\n",
    "calcu(** params)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d1cc3",
   "metadata": {},
   "source": [
    "# PART 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc6e1c2",
   "metadata": {},
   "source": [
    "\n",
    "- cross entropy\n",
    "- learning_rate_tuning(TODO)\n",
    "- adamw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475bf378",
   "metadata": {},
   "source": [
    "### adamwAccounting\n",
    "#####  a\n",
    "\n",
    "- 参数内存 = 参数数量P * 4\n",
    "- 激活内存（中间tensor）= B * L * d_model (Embedding 的)\n",
    "                        \\+ num_layer * (16 B * L * d_model + B * num_heads * L * L)\n",
    "                        \\+ B * L * vocab_size\n",
    "- 梯度内存 = 4 * P\n",
    "- 优化器状态 = 8 * P\n",
    "\n",
    "- 总内存为4个加和\n",
    "\n",
    "#####  b\n",
    "\n",
    "固定内存 = 16P \\sim 24.88 GB\n",
    "激活内存 \\sim 10.89 * B GB\n",
    "\n",
    "算出 80GB Memory下 batch最大为5\n",
    "\n",
    "\n",
    "#####  c\n",
    "\n",
    "- forward的 FLOPs = 前面算的\n",
    "- backward的 FLOPs = 2 * Forward_FLOPs = 2 * FP\n",
    "- 优化器更新的 FLOPs = 4 * P\n",
    "- 总共为 3 * FP + 4 * P\n",
    "\n",
    "#####  d\n",
    "\n",
    "- 总 FLOPs 计算\n",
    "时间(天) = 总FLOPs / 有效硬件FLOP/s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479017e",
   "metadata": {},
   "source": [
    "- cosine learning rate schedule withwarmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8903df8b",
   "metadata": {},
   "source": [
    "# PART 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38219afe",
   "metadata": {},
   "source": [
    "- save / load\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
