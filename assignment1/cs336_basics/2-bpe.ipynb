{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51203cd8",
   "metadata": {},
   "source": [
    "# Problem （unicode1）: Understanding Unicode （1 point）\n",
    "\n",
    "```python\n",
    ">>> chr(0)\n",
    "'\\x00'\n",
    ">>> print(chr(0))\n",
    "\n",
    ">>> \"this is a test\" + chr(0) + \"string\"\n",
    "'this is a test\\x00string'\n",
    ">>> print(\"this is a test\" + chr(0) + \"string\")\n",
    "this is a teststring\n",
    ">>>\n",
    "```\n",
    "\n",
    "# Problem(unicode2): Unicode Encodings(3 points)\n",
    "\n",
    "## (a)\n",
    "\n",
    "- UTF-8 为变长编码，对 ASCII 字符（文本中常见）仅用 1 字节，存储和处理更高效；\n",
    "- UTF-8 兼容 ASCII，在多语言文本中适应性更强\n",
    "- 而 UTF-16/32 对 ASCII 文本冗余度高（UTF-16 至少 2 字节，UTF-32 固定 4 字节）。\n",
    "\n",
    "## (b)\n",
    "\n",
    "输入 日语或中文即可\n",
    "\n",
    "## (c)\n",
    "\n",
    "指的是不符合 UTF-8 规则的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c121b87",
   "metadata": {},
   "source": [
    "# 2.4 BPE Tokenizer Training\n",
    "\n",
    "\n",
    "\n",
    "- Vocabulary initialization\n",
    "- pre-tokenization\n",
    "- BPE merge\n",
    "- Special tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9732c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb9195",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e756db5",
   "metadata": {},
   "source": [
    "## Problem: BPE Tokenizer Training (15 points)\n",
    "\n",
    "这里主要问题在于弄清楚类型都是什么，bytes如何操作\n",
    "\n",
    "- initial_bytes\n",
    "  - bytes([i])和 bytes(i)的差异！\n",
    "  - bytes(i)是i个\\0拼起来\n",
    "\n",
    "- _pre_tokenize\n",
    "  - 他的任务是，拆分出初步的pretokenize的结果\n",
    "  - 注意：按照special tokens 拆分开，不能跨越special token进行merge\n",
    "  - 返回pre_tokens，内容为{tuple(byte1, byte2, ... , bytek) : count}\n",
    "- train\n",
    "  - 重点是merge部分的算法，这里是主要时耗\n",
    "- 为什么采取分离的方式构建vocab?\n",
    "  - 这样只需要维护`merged`就可以了\n",
    "\n",
    "\n",
    "- 2025-10-20最新战报：最大堆修正失败，寄\n",
    "- 2025-10-28 重整旗鼓 over\n",
    "\n",
    "\n",
    "- TODO: `Optionally (this could be a large time-investment),you can implement the key parts of your training method using some systems language, for instanceC++ (considercppyyfor this) or Rust (using PyO3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5e8ec",
   "metadata": {},
   "source": [
    "## Problem （train_bpe_tinystories）: BPE Training on TinyStories （2 points）\n",
    "\n",
    "- 2025-10-29 改了下 pre-tokenize, 防止一下全放到内存里给干炸了\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f501bff",
   "metadata": {},
   "source": [
    "## Problem （train_bpe_expts_owt）: BPE Training on OpenWeb Text （2 points）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed55a6",
   "metadata": {},
   "source": [
    "# 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953be9f1",
   "metadata": {},
   "source": [
    "## Problem （tokenizer）:Implementing the tokenizer （15 points）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9d3f8",
   "metadata": {},
   "source": [
    "## Problem （tokenizer_experiments）:Experiments with tokenizers （4 points）"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
