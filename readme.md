## 先决条件

- 精通 Python

  - 大部分课程作业将使用 Python 语言完成。与大多数其他 AI 课程不同，本课程将为学生提供极少的框架。学生需要编写的代码量将比其他课程至少高出一个数量级。因此，精通 Python 和软件工程至关重要。

- 具有深度学习和系统优化经验

  - 本课程的一大重点是让神经语言模型在多台机器的 GPU 上快速高效地运行。我们希望学生能够熟练掌握 PyTorch，并了解内存层次结构等基本系统概念。

- 大学微积分、线性代数（例如 MATH 51、CME 100）

  - 您应该能够轻松理解矩阵/向量符号和运算。

- 基础概率论与统计学（例如 CS 109 或同等课程）

  - 您应该了解概率、高斯分布、平均值、标准差等基础知识。

- 机器学习（例如 CS221、CS229、CS230、CS124、CS224N）
  - 您应该熟悉机器学习和深度学习的基础知识。

请注意，本课程包含 5 个单元。由于课程内容侧重于实践，请预留充足的学习时间。

## 课程作业

- 作业 1：基础知识
  - 实现训练标准 Transformer 语言模型所需的所有组件（标记器、模型架构、优化器）。
  - 训练最小语言模型。
- 作业 2：系统
  - 使用高级工具对作业 1 中的模型和层进行分析和基准测试，使用您自己的 FlashAttention2 的 Triton 实现优化注意力。
  - 构建一个内存高效的分布式版本的作业 1 模型训练代码。
- 作业 3：扩展
  - 了解 Transformer 各部件的功能。
  - 查询训练 API 以使缩放定律适合项目模型缩放。
- 作业 4：数据
  - 将原始 Common Crawl 转储转换为可用的预训练数据。
  - 执行过滤和重复数据删除以提高模型性能。
- 作业 5：对齐和推理 RL
  - 应用监督微调和强化学习来训练 LM 在解决数学问题时进行推理。
  - 可选第 2 部分：实施和应用 DPO 等安全协调方法。
